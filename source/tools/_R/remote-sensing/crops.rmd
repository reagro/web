```{r, global_options, tidy = TRUE, echo=FALSE, message=FALSE}
library(knitr)
library(agrin)
library(randomForest)

opts_chunk$set(tidy.opts=list(width.cutoff=60))
```
# Crop mapping with satellite data

Benson Kenduiywo


## Background

Mapping the distribution of crops is important for several applications. A problem with using remote sensing data for crop mapping is the presence of clouds. As crops are typically grown in the rainy seasons, they may be difficult to detect with the commonly used optical reflectance data. Radar remote sensing data is not affected by weather conditions (or the presence of daylight), and that makes it a very interesting alternative to optical data. Here we use Sentinel-1 radar data to map different crop types in Kitale, Kenya. We build on the approaches introduced in the introduction to remote sensing tutorial, but we focus on other aspecs (feature creation and model evaluation).


TO DO: add link to page on general Sentinel info (some technical specs and where to get)


## Data

We provide a set of processed Sentinel-1 VV and VH polarized images. To learn about the basics of radar visit [EO College](https://eo-college.org/resources/basics-of-sar/), [SERVIR SAR HANDBOOK](https://gis1.servirglobal.net/TrainingMaterials/SAR/SARHB_FullRes.pdf) or [European Space Agency (ESA) tutorials](https://sentinel.esa.int/web/sentinel/toolboxes/sentinel-1/tutorials). The images are ground range products with equidistant pixel spacing. They were radiometrically calibrated to $\sigma_0$. All images were co-registered and transformed to the UTM-36N and WGS1984 datum coordinate reference system. 

Here is how you can get the Sentinel data for this tutorial. 

```{r images}
workdir <- file.path(dirname(tempdir()))
datadir <- file.path(workdir, "agrin/sentinel")
dir.create(datadir, showWarnings = FALSE, recursive=TRUE)
library(agrin)
datadir
ff <- crop_data("sentinel1", datadir)
head(ff)
```

Here is just a trick to skip some preprocessing if that has already been done
```{r haveimg}
imgfile <- file.path(workdir, "img.tif")
imgexists <- file.exists(imgfile)
```

You can use the `grep` funtion to get the VV polarized images.
```{r polvv}
fvv <- grep("VV", ff, value = TRUE)
i <- order(substr(basename(fvv), 15, 27))
fvv <- fvv[i]
fvv

vv <- rast(fvv)
# make prettier (shorter) layer names
names(vv) <- substr(names(vv), 15, 27)
names(vv)
vv
```

Repeat the steps for vh polarization.

```{r polvh}
fvh <- grep("VH", ff, value = TRUE)
i <- order(substr(basename(fvh), 15, 27))
fvh <- fvh[i]
vh <- rast(fvh)
names(vh) <- substr(names(vh), 15, 27)
names(vh)
```

We also have ground reference data on crop types.

```{r ref1}
cropref <- crop_data("crop_ref")
head(cropref)
table(cropref$class)
```

Create a SpatVector object
```{r ref2}
vecref <- vect(cropref[,1:2], att=cropref, crs=crs(vv))
vecref 
spplot(vecref, "class")
```

As you can see, the points were generated by sampling within fields.


## Feature creation

We have "raw" time series satellite data, and many more images for VV than for VH. We can use these data to build a model to predict crop types, but oftentimes we prefer to compute new "features" (predictor variables) from the raw data. These new features should typically reduce the number of variables (images), while capturing perhaps more of the variation in the data.

Here is function to compute features. It takes a multilayer object `x` to create new layers.

```{r features}
compFeatures <- function(x, name){
	# standard deviation
	stdev <- stdev(x)
	# quantiles
	quant <- app(x, function(i) quantile(i, c(.1, .25, .5, .75, .9)))
	# trend
	n <- nlyr(x)
	trend <- 100 * (mean(x[[1:5]] - mean(x[[(n-4):n]])))

	feat <- c(quant, stdev, trend)
	names(feat) <- paste0(name, c(".1", ".25", ".5", ".75", ".9", ".sd", "trend"))
	return(feat)
} 
```

Compute seasonal composite features from VV polarized images

```{r vvfeatures, fig.width=12, fig.height=12}
fvv <- file.path(workdir, "vv_ft.tif")

if (!file.exists(fvv)) {
	vv_ft <- compFeatures(vv, "vv")
	writeRaster(vv_ft, fvv)
} else {
	vv_ft <- rast(fvv)
}

vv_ft
plot(vv_ft)
```

As we only have 4 images for VH polarized images, we cannot use the function above to create features, we'll just use the mean and the standard deviation.

```{r vhfeatures, fig.width=12, fig.height=8}
fvh <- file.path(workdir, "vh_ft.tif")
if (!file.exists(fvh)) {
	sd <- stdev(vh)
	mn <- mean(vh)
	vh_ft <- c(mn, sd)
	names(vh_ft) <- c("mean", "stdev")
	writeRaster(vh_ft, fvh)
} else {
	vh_ft <-- rast(fvh)
}
plot(vh_ft)
```
Combine the `vv` and `vh` features

```{r scaling}
img <- c(vv_ft, vh_ft)
```

## Model fitting

Various supervised classification algorithms exist, and the choice of algorithm affects the results. Here we used the widely used Random Forests (RF) alogirthm ([Breiman 2001](https://link.springer.com/article/10.1023/A:1010933404324)).

Random Forests makes predictions using an ensemble of a large number of decision trees. The individual trees are uncorrelated because they are fitted with different subsets of the data. Each tree is fitted with a bootstrapped sample of all the data. A bootstrapped sample is obtained by taking a sample with replacement of the same size as the original data. RandomForest also uses a maximum user-defined number of variables that can randomly selected from to create a node. A different sample is used at each node. Thus each tree has a somewhat different data set and the "best" variable is not always avaiable to create a node. The final prediction (by the forest) is made by averaging the prediction of the individual trees. 

The user needs to set the number of trees to build. The more trees, the longer it takes to build the model and to make predictions. Generally, RandomForests models tend to be stable once there are 200 trees ([Hastie et al. (2009)](https://www.springer.com/gp/book/9780387848570)). 

To train a model, we extract the satellite data for the ground points, using the feature (predictor) data created above.

```{r getref}
ref <- extract(img, vecref, drop=TRUE)
ref <- data.frame(class=cropref$class, ref)
head(ref)
```

Now we can train the model

```{r RFtrain}
library(randomForest)
set.seed(888)
rf.m <- randomForest(class~., data=ref, ntree=250, importance=TRUE)
```

When we print the model object, we get some diagnostics. Can you understand them? What does "OOB" mean?

```{r RFprint}
rf.m
```

We can also plot the model object

```{r RFplot}
cols <- c("green", "yellow", "darkgreen", "magenta", "blue")
plot(rf.m, col=cols, lwd=3)
legend("topright", col=cols , c("OOB", "Grass", "Maize","Pasture","Wheat"), lty=1, lwd=3)
```

There is a number of other tools to inspect the model. The "Variable Importance Plot" is an important tool that can be used with any regression-like model to determine the importance of individual variables. 

```{r RFPlots}
varImpPlot(rf.m, main="")
```

The plot shows each variable on the y-axis, and their importance on the x-axis. They are ordered top-to-bottom as **most- to least-important**. There are two plots namely:

1. **Mean decrease in accuracy**. The lower the predictive performance of Random Forest due to the exclusion (randomization of the values, in fact) of particular single variable, the more important the variable is. 

2. **Mean decrease in Gini** estimates the contribution of each variable to achieve homogeneity in the groups created by the trees. Every time a variable is used to split the data (create a node), the Gini coefficient for the child nodes are estimated and compared to that of the data of the mother node. The **Gini coefficient** is a measure of homogeneity in the range of 0 (homogeneous) to 1 (heterogeneous). Variables that result in nodes with *higher purity* have a *higher decrease in Gini coefficient*.

You can also look at the raw data used to make the plot
```{r RFplotsdata}
importance(rf.m)
```

Another good diagnostic tool is the partial-plot. It shows the response to a single variable (all other variables are kept at their original values).  

```{r partialplot}
partialPlot(rf.m, ref, "vvtrend", "Maize")
```

Like the variable importance plot, the partial plot can be computed for any regression-like (or classification) model; but there is not always a simple function available to quickly do it.


## Model prediction

Now we use the model to predict the distribution of crops over the entire study region. This is the easy part.

```{r RFpredict}
rf.pred <- predict(img, rf.m)
rf.pred
```

Make a map

```{r RFmap, fig.width=6, fig.height=6}
rfp <- as.factor(rf.pred)
levels(rfp) <- c("Grassland", "Maize", "Pasture", "Wheat")
plot(rfp, col=c("green", "yellow", "darkgreen", "magenta"))
``` 


## Post classifation improvements

TO DO: show and discuss the removal of noise



## Model evaluation

It is important to evaluate predictive models. It allows comparison with alternative methods, and it can help establish some confidense in the predictions. Predictive models are generally evaluated with "cross-validation". This is a technique in which the data are split in *k* folds. Each fold is held out once to evaluate the model created with the other folds. An evaluation statistic is computed for each fold, and averaged. Typically we use 5 or 10 fold cross-validation (do not use more folds).

Let's make 5 "folds". That is, each record is assinged to one of 5 groups

```{r sample}
library(luna)
set.seed(530)
k <- kfold(ref, 5)
nrow(ref) / 5
table(k)
```

We now would need a loop to go over each fold, but here we just show the approach for a single fold.

Get training and testing samples
```{r split}
train <- ref[k != 1, ]
test  <- ref[k == 1, ]
```

Build a new model
```{r predfit}
rf <- randomForest(class~., data=train, ntree=250, importance=TRUE)
```

And use the model to make a prediction for the test data. Importantly, the model did not see the test data!

```{r predtest}
pred <- predict(rf, test)
```

And create a confusion matrix

```{r accuracy}
conmat <- table(observed=test$class, predicted=pred)
conmat
```

Compute one or more statistics

```{r corclas}
# overall accuracy
evaluate(conmat, "overall")
# kappa
evaluate(conmat, "kappa")
# user and producer accuracy
evaluate(conmat, "class")
```



### Other algorithms

1) Use the same data with another algorithm and compare the resuls. 



## References

Breiman, L., 2001. Random forests. *Machine learning*, 45(1), 5-32.

James, G., D. Witten, T. Hastie and R. Tibshirani, 2013. [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/). Springer.


---
This document was last generated on `r as.Date(Sys.time())`.
